# LLM
## [ML](https://zhuanlan.zhihu.com/p/74673610)
> 机器学习(Machine Learning，ML)是指从有限的观测数据中学习(或“猜测”)出具有一般性的规律，并将这些规律应用到未观测数据样本上的方法

机器学习三个基本要素：模型、学习准则和优化方法

机器学习的目标是找到一个模型来近似真实映射函数$g(x)$或真实条件概率分布 $p (y|x)$。 由于这个函数是未知的,所以只能根据经验来确定一个假设函数集合F，称为假设空间(Hypothesis Space)， 然后通过观测其在训练集 D 上的特性，从中选择一个理想的假设(Hypothesis) $f^* \in F$。 假设空间 F 通常为一个参数化的函数族$F = {f(x;θ)|θ \in R^m}$,其中 $f (x; θ)$ 为假设空间中的模型，θ 为一组可学习参数，m 为参数的数量。

### 题目
- 基本模型有啥，是干什么的
- 各个模型为了解决什么东西
- 机器学习的训练测试验证集合是什么，怎么用的
- 机器学习的输入是什么？输出是什么？
- 模型训练是在干什么事？
- 为什么机器学习是在”训练“模型
- 什么是欠拟合和过拟合
- 机器学习的训练目标是什么
- 监督无监督和强化是什么


## NLP
> 预处理（preprocess） -> 分词（Tokenization） -> 模型优化（optimization） -> 模型（会拿transformer举例） -> 精调（fine tuning）

### [预处理(preprocess)](https://zhuanlan.zhihu.com/p/619241179)
机器学习极其依赖数据的质量，因此在训练前数据的预处理永远都是最关键的一步

LLaMA这个工作的贡献之一就是证明了只用开源数据也能训练出decent performance的LLMs。这可以算得上是对OpenAI花大价钱搞独家数据这一都市传说的回应。事实上只要开源数据保证数量和质量，LLMs的表现不会在数据方面出现瓶颈(至少当下还没有)。

总结来看，数据清洗可以分为以下几点：

去重：去重是防止模型过拟合的格外重要的一步。
去低质量内容：低质量文本对模型效果，和训练进度都有很大的负面影响。
解析.html/.tex文件，取得纯文本：这个不用解释，就是取得干净的文本内容必须的过程
合规方面考虑：包括考虑数据源的license，过滤有毒信息，个人隐私信息等内容。

#### 题目
- 预处理做了什么？
- C4、CommonCrawl等开源数据源是什么，如果有条件可以下载一些浏览看看
- 这些数据是如何应用到模型的训练中的

### 分词(Tokenization)

> [词嵌入是什么](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E8%AF%8D%E5%B5%8C%E5%85%A5word-embedding-2%E7%A7%8D%E7%AE%97%E6%B3%95-%E5%85%B6%E4%BB%96%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%AF%94%E8%BE%83-c7dd8e4524db)
>
> [byte pair encoding分词](https://www.cnblogs.com/zjuhaohaoxuexi/p/16412976.html)
>
> [word piece分词](https://huggingface.co/learn/llm-course/chapter6/6?fw=pt)
>
> [Unigram Language Model分词](https://huggingface.co/learn/llm-course/chapter6/7?fw=pt)
>
> [why padding](https://medium.com/@canerkilinc/padding-for-nlp-7dd8598c916a)

分词，简而言之就是把一句话分成多个词组，例如我是一个人类，我喜欢水果可以被分词为["我", "是", "一个", "人类，", "我", "喜欢", "水果"]。

由于机器是无法直接理解人类语言的，因此人类语言首先要被转化为机器可以理解的向量（这个向量维度越高，所包含的信息也就越多），而在自然语言，词是最小的单位，如果直接把一整句话转化为向量便会丢失大量的信息，所以我们需要首先把句子拆分为词组，然后再对每个词转化为向量，最后将一句话的所有向量拼接成矩阵（例如一个句子分词后有N个词，每个词的向量维度为M，那么就可以组成一个N*M的矩阵）

#### 题目
- 分词干了什么事？
- 为什么需要把句子分词？
- bpe、wordpiece和unigram language model这三种分词模型的原理是什么？
- 什么是词嵌入
- 如何解决不同长度句子的转化为矩阵后的维度不一致的问题？







未完待续~~~