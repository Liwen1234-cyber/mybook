## 线性映射的矩阵表示

给定 $\mathbb {F}$ 上的线性空间 $V_1, V_2$，及线性映射 $\mathscr {A}:V_1\to V_2$。设 $\dim (V_1)=n, \dim (V_2)=m$，并设 $\varepsilon_1,\varepsilon_2,...,\varepsilon_n$ 为 $V_1$ 的一个基（称为入口基）；$\eta_1, \eta_2,...,\eta_m$ 为 $V_2$ 的一个基（称为出口基）。记第 $j$ 个入口基向量 $\varepsilon_j \in V_1$ 在 $\mathscr {A}$ 下的像 $\mathscr {A}(\varepsilon_j)\in V_2$ 在出口基 $\eta_1, \eta_2,...,\eta_n$ 下的坐标为

$$
a_j = \begin{bmatrix}a_{1j} \\ a_{2j}\\ \vdots \\ a_{mj}\end{bmatrix}\in \mathbb{F}^m
$$

即

$$
\mathscr{A}(\varepsilon_j)=\begin{bmatrix}\eta_1 & \eta_2 & \cdots & \eta_m\end{bmatrix}\begin{bmatrix}a_{1j}\\ a_{2j}\\ \vdots \\ a_{mj}\end{bmatrix},\ j=1,2,...,n
$$

则由 $\mathbb {F}^m$ 中的向量组 $a_1, a_2,...,a_n$ 拼成的矩阵

$$
A = \begin{bmatrix}a_1 & a_2 & \cdots & a_n\end{bmatrix} = [a_{ij}]_{m\times n}
$$

称为 $\mathscr {A}$ 在相应的入口基和出口基下的表示

定义记号

$$
\mathscr{A}\begin{bmatrix}\varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_n\end{bmatrix}=\begin{bmatrix}\mathscr{A}(\varepsilon_1) & \mathscr{A}(\varepsilon_2) & \cdots & \mathscr{A}(\varepsilon_n)\end{bmatrix}
$$

即线性映射作用在向量组拼成的矩阵上，定义为向量组中每个向量的像按原顺序所成的向量组（简称为向量组的像）拼成的矩阵。则有下面的公式

$$
{\color{red} {\mathscr{A}\begin{bmatrix}\varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_n\end{bmatrix}=\begin{bmatrix}\eta_1 & \eta_2 & \cdots & \eta_m\end{bmatrix}A}}
$$

用文字表示，读作

$$
[线性映射][入口基矩阵]=[出口基矩阵][表示矩阵]
$$

事实上，只要确定了线性映射两个空间的**基**（例如 $(\varepsilon_1,\cdots,\varepsilon_n)$ 和 $(\beta_1,\cdots,\beta_m)$），就有唯一确定的一个矩阵 $A$ 与之对应，而且**矩阵 $A$ 的每一个列向量就是对应的原基向量映射后的坐标**；反之，如果基确定，任何一个矩阵都唯一确定了一个线性映射

> 我个人理解，线性映射其实就是将一个 $m$ 维的矩阵，转换为 $n$ 维的矩阵，而在转换过程中，需要一个 $m\times n$ 的矩阵 $A$，这类似于 PyTorch 中的 `nn.Linear(m, n, bias=False)` 函数

![](./images/792497264.png)

___

**用坐标计算线性映射**

设线性映射 $\mathscr {A}:V_1\to V_2$ 在入口基 $\varepsilon_1, \varepsilon_2,...,\varepsilon_n$ 和出口基 $\eta_1, \eta_2,...,\eta_m$ 下的矩阵表示为 $A$，又设 $\alpha \in V_1$ 在入口基下的坐标为 $X \in \mathbb {F}^n$，则 $\mathscr {A}(\alpha)\in V_2$ 在出口基下的坐标为 $AX\in \mathbb {F}^m$

**证明：**也就是要证 $\mathscr {A}(\alpha)=\begin {bmatrix}\eta_1 & \eta_2 & \cdots & \eta_m\end {bmatrix}(AX)$

因为

$$
\begin{aligned}
\begin{bmatrix}\eta_1 & \eta_2 & \cdots & \eta_m\end{bmatrix}(AX)&=
(\begin{bmatrix}\eta_1 & \eta_2 & \cdots & \eta_m\end{bmatrix}A)X\\
&=(\mathscr{A}\begin{bmatrix}\varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_n\end{bmatrix})X\\
&=\mathscr{A}(\begin{bmatrix}\varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_n\end{bmatrix}X)\\
&=\mathscr{A}(\alpha)
\end{aligned}
$$

证毕 